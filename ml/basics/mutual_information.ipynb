{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124ad0a8",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "\n",
    "- StatQuest Video: https://youtu.be/eJIp_mgVLwE\n",
    "\n",
    "Mutual Information (MI) is a measure from information theory that quantifies the dependency between two random variables. In data science, it's a powerful tool for **feature selection** because it tells us how much information a feature provides about the **target variable** we want to predict.\n",
    "\n",
    "**Main Idea:** Mutual Information is a numeric value that gives us a sense of how closely related two variables are.\n",
    "*   **A value of 0** means the variables are independent; knowing one tells you nothing about the other.\n",
    "*   **A higher value** means the variables are more related; the changes in one variable correspond to predictable changes in the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f6b1",
   "metadata": {},
   "source": [
    "### The Mutual Information Formula\n",
    "\n",
    "The mathematical formula for Mutual Information, as presented in the video, is:\n",
    "\n",
    "$$\n",
    "\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\left[ \\frac{p(x, y)}{p(x)p(y)} \\right]\n",
    "$$\n",
    "\n",
    "The Mutual Information formula is built on two fundamental concepts from probability:\n",
    "\n",
    "1.  **Joint Probability:** This is the probability of two events happening at the same time.\n",
    "    *   **Video Terminology:** `p(x, y)`\n",
    "    *   **Example:** The probability that someone both **Likes Popcorn** (`Yes`) *and* **Loves Troll 2** (`Yes`).\n",
    "\n",
    "2.  **Marginal Probability:** This is the probability of a single event occurring, regardless of the other events.\n",
    "    *   **Video Terminology:** `p(x)` or `p(y)`\n",
    "    *   **Example:** The overall probability that someone **Loves Troll 2** (`Yes`), without considering whether they like popcorn or not.\n",
    "    *   **Why \"Marginal\"?** In a probability table, these values are the totals found in the *margins* (the sum of a row or column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d025445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Likes Popcorn</th>\n",
       "      <th>Height (m)</th>\n",
       "      <th>Loves Troll 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.77</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.32</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.81</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>1.56</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>1.64</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Likes Popcorn  Height (m) Loves Troll 2\n",
       "0           Yes        1.77           Yes\n",
       "1           Yes        1.32           Yes\n",
       "2           Yes        1.81           Yes\n",
       "3            No        1.56            No\n",
       "4            No        1.64           Yes"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Likes Popcorn': ['Yes', 'Yes', 'Yes', 'No', 'No'],\n",
    "    'Height (m)': [1.77, 1.32, 1.81, 1.56, 1.64],\n",
    "    'Loves Troll 2': ['Yes', 'Yes', 'Yes', 'No', 'Yes'] # Target variable\n",
    "}\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947919c",
   "metadata": {},
   "source": [
    "## Calculating Mutual Information\n",
    "\n",
    "Before calculating the MI score, we first organize the probabilities from our dataset into a table. The four inner cells contain the **Joint Probabilities**, and the totals in the margins contain the **Marginal Probabilities**.\n",
    "\n",
    "|                            | **Likes Popcorn: Yes** | **Likes Popcorn: No** | **Row Totals (Marginal P(Troll 2))** |\n",
    "| :------------------------- | :--------------------: | :-------------------: | :--------------------------------: |\n",
    "| **Loves Troll 2: Yes**     |          3/5           |          1/5          |                **4/5**                 |\n",
    "| **Loves Troll 2: No**      |          0/5           |          1/5          |                **1/5**                 |\n",
    "| **Column Totals (Marginal P(Popcorn))**|          **3/5**           |          **2/5**          |                  1                   |\n",
    "\n",
    "The video walks through a clear example using the variables `Likes Popcorn` and `Loves Troll 2`.\n",
    "\n",
    "1.  **Create a Probability Table:**\n",
    "    *   Set up a table like the one above. The values in this table are all you need for the formula.\n",
    "\n",
    "2.  **Plug into the Formula:**\n",
    "    The Mutual Information formula sums a value for every possible combination of outcomes (i.e., for each of the four inner cells in the table). For each term in the summation:\n",
    "    *   `p(x, y)` is the **joint probability** from an inner cell.\n",
    "    *   `p(x)p(y)` is the product of the two corresponding **marginal probabilities** from the margins.\n",
    "    *   The formula calculates how different the actual joint probability is from what we would expect if the two variables were independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70e6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the Mutual Information between two discrete variables.\n",
    "\n",
    "    This function follows the methodology from the StatQuest video:\n",
    "    1. It identifies all unique value combinations.\n",
    "    2. It calculates joint and marginal probabilities for each combination.\n",
    "    3. It sums the terms of the MI formula: Î£ p(x,y) * log( p(x,y) / (p(x)*p(y)) )\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): The first variable (e.g., a column from a DataFrame).\n",
    "        y (pd.Series): The second variable.\n",
    "\n",
    "    Returns:\n",
    "        float: The Mutual Information score.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    x_values = x.unique()\n",
    "    y_values = y.unique()\n",
    "    \n",
    "    mi_score = 0\n",
    "\n",
    "    for y_val in y_values:\n",
    "        for x_val in x_values:\n",
    "            \n",
    "            p_x = (x == x_val).sum() / n\n",
    "            p_y = (y == y_val).sum() / n\n",
    "            p_xy = ((x == x_val) & (y == y_val)).sum() / n\n",
    "\n",
    "            if p_xy > 0:\n",
    "                mi_score += p_xy * np.log(p_xy / (p_x * p_y))\n",
    "\n",
    "    return mi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5aa695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mutual Information between 'Likes Popcorn' and 'Loves Troll 2' is: 0.2231\n"
     ]
    }
   ],
   "source": [
    "mi_score_scratch = mutual_information(data['Likes Popcorn'], data['Loves Troll 2'])\n",
    "\n",
    "# 0.22, same as what's shown in the video\n",
    "print(f\"The Mutual Information between 'Likes Popcorn' and 'Loves Troll 2' is: {mi_score_scratch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1dc4ea",
   "metadata": {},
   "source": [
    "### Handling Continuous Variables\n",
    "\n",
    "Mutual Information isn't limited to discrete (categorical) variables.\n",
    "\n",
    "*   **Problem:** A variable like `Height` is continuous and doesn't have simple categories like \"Yes\" or \"No\".\n",
    "*   **Solution:** **Discretize the continuous variable.** This means converting it into a set of bins or categories. For example, you can create a histogram for `Height` and treat each bin as a discrete category (e.g., \"Bin #1\", \"Bin #2\", etc.).\n",
    "    *   Once the continuous variable is binned, you can calculate the joint and marginal probabilities just as you would with two discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7939f02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    0\n",
       "2    2\n",
       "3    1\n",
       "4    1\n",
       "Name: Height Binned, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bins = 3\n",
    "data['Height Binned'] = pd.cut(\n",
    "    x=data['Height (m)'],       # The data to bin\n",
    "    bins=n_bins,                # The number of bins to create\n",
    "    labels=False,               # IMPORTANT: This gives us integer labels (0, 1, 2)\n",
    "    include_lowest=True         # Ensures the smallest value is included in the first bin\n",
    ")\n",
    "\n",
    "data['Height Binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6995c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mutual Information between 'Height (m)' and 'Loves Troll 2' is: 0.2231\n"
     ]
    }
   ],
   "source": [
    "mi_score_scratch = mutual_information(data['Height Binned'], data['Loves Troll 2'])\n",
    "\n",
    "# 0.22, same as what's shown in the video\n",
    "print(f\"The Mutual Information between 'Height (m)' and 'Loves Troll 2' is: {mi_score_scratch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb1200",
   "metadata": {},
   "source": [
    "## Relevant Scikit-Learn Components\n",
    "\n",
    "Scikit-learn provides powerful tools to calculate Mutual Information for feature selection, saving you from doing the math by hand. The main module is `sklearn.feature_selection`.\n",
    "\n",
    "1.  **`mutual_info_classif`**:\n",
    "    *   **What it is:** A function that estimates the mutual information between each feature and a discrete (categorical) target variable.\n",
    "    *   **Relevance:** This is the most direct implementation of the concept shown in the video for a classification problem (like predicting `Loves Troll 2`). It automatically handles continuous features by using a sophisticated form of binning (based on nearest neighbors), so you don't have to discretize them manually.\n",
    "\n",
    "2.  **`mutual_info_regression`**:\n",
    "    *   **What it is:** A similar function that estimates the mutual information between each feature and a continuous target variable.\n",
    "    *   **Relevance:** This is what you would use if your target variable was something like `Height` or `Price` instead of a \"Yes/No\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9369a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mutual Information between 'Likes Popcorn' and 'Loves Troll 2' is: 0.2231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "x = data['Likes Popcorn'].map({'Yes': 1, 'No': 0})\n",
    "y = data['Loves Troll 2'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Step: Reshape the feature data 'x' into a 2D array\n",
    "# Scikit-learn expects the feature data (X) to be in the format [n_samples, n_features].\n",
    "# Since we have only one feature, it should be a column vector.\n",
    "X_reshaped = x.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Step: Call the function with the `discrete_features=True` parameter\n",
    "# This tells the function to treat our feature as discrete/categorical,\n",
    "# which is the key to getting the correct result on this small dataset.\n",
    "mi_score = mutual_info_classif(\n",
    "    X_reshaped, \n",
    "    y, \n",
    "    discrete_features=True, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# The result is an array with one score, so we access it with [0]\n",
    "final_score = mi_score[0]\n",
    "\n",
    "print(f\"The Mutual Information between 'Likes Popcorn' and 'Loves Troll 2' is: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a0a2d",
   "metadata": {},
   "source": [
    "*   Scikit-learn's result for `Height (m)` would be `0.0000`, and not `0.2223`.\n",
    "*   **Why the difference?** Scikit-learn's `mutual_info_classif` does **not** use simple histogram binning for continuous data. It uses a more sophisticated and robust method based on k-nearest neighbors (k-NN) to estimate entropy without explicit binning. This method is generally better as it doesn't depend on an arbitrary number of bins, but it's more complex than the intuitive binning approach taught in the StatQuest video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
